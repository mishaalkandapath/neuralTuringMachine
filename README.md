# neuralTuringMachine
## Component1: Tranformer
The attention mechanism takes three entities into account, queries (Q), keys (K), and values (V). Q corresponds to an encoding from the decoder's hidden state, whereas K and V are from the encoders hidden state. The idea is to produce a weighting (using softmax) to decide what part of the encoder's hidden state (inputs) the decoders next output should focus on (attention lol). 
![attn.png](https://github.com/mishaalkandapath/neuralTuringMachine/notes/attn.png)
